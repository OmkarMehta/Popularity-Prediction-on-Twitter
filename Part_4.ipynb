{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Prediction on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dictionary_train = {'#GoHawks' : ['tweets_#gohawks.txt', 188136],\n",
    "                          '#GoPatriots' : ['tweets_#gopatriots.txt', 26232],\n",
    "                          '#NFL' : ['tweets_#nfl.txt', 259024],\n",
    "                          '#Patriots' : ['tweets_#patriots.txt', 489713],\n",
    "                          '#SB49' : ['tweets_#sb49.txt', 826951],\n",
    "                          '#SuperBowl' : ['tweets_#superbowl.txt', 1348767]}\n",
    "    \n",
    "files_dictionary_test = {1 : ['sample1_period1.txt', 730],\n",
    "                         2 : ['sample2_period2.txt', 212273],\n",
    "                         3 : ['sample3_period3.txt', 3628],\n",
    "                         4 : ['sample4_period1.txt', 1646],\n",
    "                         5 : ['sample5_period1.txt', 2059],\n",
    "                         6 : ['sample6_period2.txt', 205554],\n",
    "                         7 : ['sample7_period3.txt', 528],\n",
    "                         8 : ['sample8_period1.txt', 229],\n",
    "                         9 : ['sample9_period2.txt', 11311],\n",
    "                        10 : ['sample10_period3.txt', 365]}\n",
    "\n",
    "#----------------------Function to extract the data from the file--------------\n",
    "def extract_info(filename_key, file_name_dictionary, is_test_data):                         \n",
    "                    \n",
    "    #-----------------------To extract the data from the file------------------                \n",
    "    time_stamps = [0]*file_name_dictionary[filename_key][1]\n",
    "    is_retweet = [False]*file_name_dictionary[filename_key][1]\n",
    "    followers_of_users = [0]*file_name_dictionary[filename_key][1]\n",
    "    \n",
    "    no_of_url_citations = [0]*file_name_dictionary[filename_key][1]\n",
    "    usernames = ['']*file_name_dictionary[filename_key][1]\n",
    "    no_of_mentions = [0]*file_name_dictionary[filename_key][1]\n",
    "    ranking_scores = [0.0]*file_name_dictionary[filename_key][1]\n",
    "    no_of_hashtags = [0]*file_name_dictionary[filename_key][1]\n",
    "    \n",
    "    location_path = ''\n",
    "    feature_name = ''\n",
    "    \n",
    "    if is_test_data:\n",
    "        location_path = './Test_data/'+file_name_dictionary[filename_key][0]\n",
    "        feature_name = 'firstpost_date'\n",
    "    else:\n",
    "        location_path = './Training_data/'+file_name_dictionary[filename_key][0]\n",
    "        feature_name = 'citation_date'\n",
    "        \n",
    "    file_in = open(location_path, encoding = \"utf8\")\n",
    "    for (line, index) in zip(file_in, range(0, file_name_dictionary[filename_key][1])):\n",
    "        data = json.loads(line)\n",
    "        time_stamps[index] = data[feature_name]\n",
    "        followers_of_users[index] = data['author']['followers']\n",
    "\n",
    "        username = data['author']['nick']\n",
    "        original_username = data['original_author']['nick']\n",
    "        if username != original_username:\n",
    "            is_retweet[index] = True\n",
    "\n",
    "        no_of_url_citations[index] = len(data['tweet']['entities']['urls'])\n",
    "        usernames[index] = username\n",
    "        no_of_mentions[index] = len(data['tweet']['entities']['user_mentions'])\n",
    "        ranking_scores[index] = data['metrics']['ranking_score']\n",
    "        no_of_hashtags[index] = data['title'].count('#')\n",
    "                \n",
    "    file_in.close()\n",
    "    \n",
    "    #--------------------To calculate the related parameters-------------------\n",
    "    start_time = min(time_stamps)\n",
    "    \n",
    "    if is_test_data:\n",
    "        start_time = (min(time_stamps)/3600)*3600\n",
    "\n",
    "    hrs_passed = int((max(time_stamps)-start_time)/3600)+1\n",
    "    hr_no_of_tweets = [0] * hrs_passed\n",
    "    hr_no_of_retweets = [0] * hrs_passed\n",
    "    hr_sum_of_followers = [0] * hrs_passed\n",
    "    hr_max_no_of_followers = [0] * hrs_passed\n",
    "    hr_time_of_the_day = [0] * hrs_passed\n",
    "    hr_no_of_url_citations = [0] * hrs_passed\n",
    "    hr_no_of_users = [0] * hrs_passed\n",
    "    hr_user_set = [0] * hrs_passed\n",
    "    hr_no_of_mentions = [0] * hrs_passed\n",
    "    hr_total_ranking_scores = [0.0] * hrs_passed\n",
    "    hr_no_of_hashtags = [0] * hrs_passed\n",
    "    for i in range(0, hrs_passed):\n",
    "        hr_user_set[i] = set([])\n",
    "        \n",
    "    for i in range(0, file_name_dictionary[filename_key][1]):\n",
    "        current_hr = int((time_stamps[i]-start_time)/3600)\n",
    "        \n",
    "        if is_retweet[i]:\n",
    "            hr_no_of_retweets[current_hr] += 1\n",
    "    \n",
    "        if followers_of_users[i] > hr_max_no_of_followers[current_hr]:\n",
    "            hr_max_no_of_followers[current_hr] = followers_of_users[i]\n",
    "\n",
    "        hr_sum_of_followers[current_hr] += followers_of_users[i]\n",
    "        hr_no_of_tweets[current_hr] += 1\n",
    "        hr_no_of_url_citations[current_hr] += no_of_url_citations[i]\n",
    "        hr_user_set[current_hr].add(usernames[i])\n",
    "        hr_no_of_mentions[current_hr] += no_of_mentions[i]\n",
    "        hr_total_ranking_scores[current_hr] += ranking_scores[i]\n",
    "        hr_no_of_hashtags[current_hr] += no_of_hashtags[i]\n",
    "\n",
    "    for i in range(0, len(hr_user_set)):\n",
    "        hr_no_of_users[i] = len(hr_user_set[i])\n",
    "    \n",
    "    if is_test_data:\n",
    "        for i in range(0, len(hr_time_of_the_day)):\n",
    "            hr_time_of_the_day[i] = ((start_time-1421222400)/3600+i)%24\n",
    "    else:\n",
    "        for i in range(0, len(hr_time_of_the_day)):\n",
    "            hr_time_of_the_day[i] = i%24\n",
    "   \n",
    "    #------------------To build the DataFrame and save it to file--------------\n",
    "    target = hr_no_of_tweets[1:]\n",
    "    target.append(0)\n",
    "    data = np.array([hr_no_of_tweets,\n",
    "                     hr_no_of_retweets,\n",
    "                     hr_sum_of_followers,\n",
    "                     hr_max_no_of_followers,\n",
    "                     hr_time_of_the_day,\n",
    "                     hr_no_of_url_citations,\n",
    "                     hr_no_of_users,\n",
    "                     hr_no_of_mentions,\n",
    "                     hr_total_ranking_scores,\n",
    "                     hr_no_of_hashtags,\n",
    "                     target])\n",
    "    data = np.transpose(data)\n",
    "    \n",
    "    data_frame = DataFrame(data)\n",
    "    data_frame.columns = ['no_of_tweets', \n",
    "                          'no_of_retweets', \n",
    "                          'sum_of_followers',\n",
    "                          'max_no_of_followers',\n",
    "                          'time_of_day',\n",
    "                          'no_of_URLs',\n",
    "                          'no_of_users',\n",
    "                          'no_of_mentions',\n",
    "                          'ranking_score',\n",
    "                          'no_of_hashtags',\n",
    "                          'target']\n",
    "\n",
    "    if os.path.isdir('./Extracted_data'):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir('./Extracted_data')\n",
    "           \n",
    "    if is_test_data:\n",
    "        data_frame.to_csv('./Extracted_data/pred_test_'+file_name_dictionary[filename_key][0][:-4]+'.csv', index = False)  \n",
    "    else:\n",
    "        data_frame.to_csv('./Extracted_data/pred_test_'+filename_key+'.csv', index = False)  \n",
    "#------------------------------------------------------------------------------  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------One-hot Encoding----------------------------------\n",
    "def one_hot_encode(data_frame):\n",
    "    time_of_day_set = range(0,24)\n",
    "    for time_of_day in time_of_day_set:\n",
    "        time_of_day_column_to_add = []\n",
    "        for time_of_day_item in data_frame['time_of_day']:\n",
    "            if time_of_day_item == time_of_day:\n",
    "                time_of_day_column_to_add.append(1)\n",
    "            else:\n",
    "                time_of_day_column_to_add.append(0)\n",
    "        data_frame.insert(data_frame.shape[1]-1,\n",
    "                  str(time_of_day)+'th_hour',\n",
    "                  time_of_day_column_to_add)\n",
    "    return data_frame\n",
    "#------------------------------------------------------------------------------  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------------------Function to perform Cross Validation---------------------\n",
    "def regression(train_hashtag, test_data_index):\n",
    "    train_x = pd.read_csv('./Extracted_data/pred_test_'+train_hashtag+'.csv')\n",
    "    test_x = pd.read_csv('./Extracted_data/pred_test_'+files_dictionary_test[test_data_index][0][:-4]+'.csv')\n",
    "    \n",
    "    train_x = one_hot_encode(train_x)\n",
    "    test_x = one_hot_encode(test_x)\n",
    "         \n",
    "    #----------------------------Splitting the data----------------------------   \n",
    "    train_x.drop('time_of_day', 1, inplace = True)\n",
    "    train_y = train_x.pop('target')\n",
    "    \n",
    "    test_x.drop('time_of_day', 1, inplace = True)\n",
    "    test_y = test_x.pop('target')\n",
    "    \n",
    "    train_x_before = train_x[:440]\n",
    "    train_x_during = train_x[440:452]\n",
    "    train_x_after = train_x[452:]\n",
    "        \n",
    "    train_y_before = train_y[:440]\n",
    "    train_y_during = train_y[440:452]\n",
    "    train_y_after = train_y[452:]\n",
    "    \n",
    "    #---------------------------Regression Prediction--------------------------\n",
    "    reg_before = RandomForestRegressor(n_estimators = 20, max_depth = 9)\n",
    "    reg_during = RandomForestRegressor(n_estimators = 20, max_depth = 9)\n",
    "    reg_after = RandomForestRegressor(n_estimators = 20, max_depth = 9)\n",
    "\n",
    "    reg_before.fit(train_x_before,train_y_before)\n",
    "    reg_during.fit(train_x_during,train_y_during)\n",
    "    reg_after.fit(train_x_after,train_y_after)\n",
    "    \n",
    "    predicted_y = []\n",
    "    if files_dictionary_test[test_data_index][0][-5] == '1':\n",
    "        predicted_y = reg_before.predict(test_x)\n",
    "    elif files_dictionary_test[test_data_index][0][-5] == '2':\n",
    "        predicted_y = reg_during.predict(test_x)\n",
    "    else:\n",
    "        predicted_y = reg_after.predict(test_x)\n",
    "    \n",
    "    #-------------------------To print the predicted values--------------------   \n",
    "    data = np.array([predicted_y, test_y])\n",
    "    data = np.transpose(data)\n",
    "    results = DataFrame(data)\n",
    "    results.columns = ['Predicted', 'Actual']\n",
    "     \n",
    "    #--------------To calculate the average cross-validation error-------------\n",
    "    total_error = 0.0\n",
    "    for i in range(len(test_y)-1):\n",
    "        total_error += abs(test_y[i] - predicted_y[i])\n",
    "    \n",
    "    return results, total_error/(len(test_y)-1)\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_data_index):\n",
    "    result_list = []\n",
    "    error_list = []\n",
    "    hashtag_list = ['#GoHawks',\n",
    "                    '#GoPatriots',\n",
    "                    '#NFL',\n",
    "                    '#Patriots',\n",
    "                    '#SB49',\n",
    "                    '#SuperBowl']\n",
    "\n",
    "    for hashtag in hashtag_list:\n",
    "        result, error = regression(hashtag, test_data_index)\n",
    "        result_list.append(result)\n",
    "        error_list.append(error)\n",
    "    \n",
    "    min_index = 0\n",
    "    min_error = error_list[0]\n",
    "    \n",
    "    for i in range(0, len(error_list)):\n",
    "        if error_list[i] < min_error:\n",
    "            min_error = error_list[i]\n",
    "            min_index = i\n",
    "    \n",
    "    return result_list[min_index],min_error,hashtag_list[min_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Optimal result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_result(test_data_index):\n",
    "    optimal_result, min_error, optimal_hashtag = predict(test_data_index)\n",
    "    \n",
    "    for i in range(20):\n",
    "        result, error, hashtag = predict(test_data_index)\n",
    "        if error  < min_error:\n",
    "            min_error = error\n",
    "            optimal_result = result\n",
    "            optimal_hashtag = hashtag\n",
    "\n",
    "    print('------------------------------------------------') \n",
    "    print('\\n'+files_dictionary_test[test_data_index][0]+'\\n')\n",
    "    print('Best training dataset:',optimal_hashtag)\n",
    "    print(optimal_result)\n",
    "    print('Average cross-validation error:',min_error)\n",
    "    print('------------------------------------------------')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_info('#GoHawks',files_dictionary_train,False)\n",
    "extract_info('#GoPatriots',files_dictionary_train,False)\n",
    "extract_info('#NFL',files_dictionary_train,False)\n",
    "extract_info('#Patriots',files_dictionary_train,False)\n",
    "extract_info('#SB49',files_dictionary_train,False)\n",
    "extract_info('#SuperBowl',files_dictionary_train,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "\n",
      "sample1_period1.txt\n",
      "\n",
      "Best training dataset: #GoPatriots\n",
      "    Predicted  Actual\n",
      "0  141.110000    82.0\n",
      "1   92.850909    68.0\n",
      "2   65.500909    94.0\n",
      "3  166.110000   171.0\n",
      "4  209.660000   178.0\n",
      "5  207.210000     0.0\n",
      "Average cross-validation error: 29.802\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "\n",
      "sample2_period2.txt\n",
      "\n",
      "Best training dataset: #SuperBowl\n",
      "   Predicted   Actual\n",
      "0   15047.65   9361.0\n",
      "1   14977.85  10374.0\n",
      "2   25317.15  20066.0\n",
      "3   45292.40  81958.0\n",
      "4   93018.70  82923.0\n",
      "5   95139.30      0.0\n",
      "Average cross-validation error: 12460.59\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "\n",
      "sample3_period3.txt\n",
      "\n",
      "Best training dataset: #GoHawks\n",
      "   Predicted  Actual\n",
      "0     458.35   549.0\n",
      "1     528.45   610.0\n",
      "2     522.55   888.0\n",
      "3     557.75   616.0\n",
      "4     522.55   523.0\n",
      "5     490.45     0.0\n",
      "Average cross-validation error: 119.27000000000001\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "\n",
      "sample4_period1.txt\n",
      "\n",
      "Best training dataset: #GoPatriots\n",
      "    Predicted  Actual\n",
      "0  334.350000   255.0\n",
      "1  236.366667   236.0\n",
      "2  246.916667   266.0\n",
      "3  237.966667   267.0\n",
      "4  237.966667   201.0\n",
      "5  152.069792     0.0\n",
      "Average cross-validation error: 32.96\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "\n",
      "sample5_period1.txt\n",
      "\n",
      "Best training dataset: #GoPatriots\n",
      "    Predicted  Actual\n",
      "0  328.450000   505.0\n",
      "1  337.300000   352.0\n",
      "2  342.000000   359.0\n",
      "3  306.700000   282.0\n",
      "4  254.200000   210.0\n",
      "5  270.787857     0.0\n",
      "Average cross-validation error: 55.42999999999999\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "\n",
      "sample6_period2.txt\n",
      "\n",
      "Best training dataset: #Patriots\n",
      "   Predicted   Actual\n",
      "0   14841.60  12943.0\n",
      "1   33417.50  60627.0\n",
      "2   36642.65  52695.0\n",
      "3   35148.70  41016.0\n",
      "4   34387.70  37293.0\n",
      "5   32503.65      0.0\n",
      "Average cross-validation error: 10786.61\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "\n",
      "sample7_period3.txt\n",
      "\n",
      "Best training dataset: #Patriots\n",
      "    Predicted  Actual\n",
      "0  101.286607   102.0\n",
      "1   70.267579    66.0\n",
      "2   59.319581    60.0\n",
      "3   56.768587    55.0\n",
      "4   46.285766   120.0\n",
      "5   93.854031     0.0\n",
      "Average cross-validation error: 16.228842255305672\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "\n",
      "sample8_period1.txt\n",
      "\n",
      "Best training dataset: #GoHawks\n",
      "   Predicted  Actual\n",
      "0  53.641470    72.0\n",
      "1  53.483812    56.0\n",
      "2  36.995872    41.0\n",
      "3  30.326448    11.0\n",
      "4  31.144610     0.0\n",
      "Average cross-validation error: 11.051323608885266\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "\n",
      "sample9_period2.txt\n",
      "\n",
      "Best training dataset: #NFL\n",
      "   Predicted  Actual\n",
      "0    1520.45  1734.0\n",
      "1    1516.55  1619.0\n",
      "2    1549.25  1582.0\n",
      "3    1875.80  1857.0\n",
      "4    3177.45  2790.0\n",
      "5    5693.55     0.0\n",
      "Average cross-validation error: 150.99999999999994\n",
      "------------------------------------------------\n",
      "------------------------------------------------\n",
      "\n",
      "sample10_period3.txt\n",
      "\n",
      "Best training dataset: #SB49\n",
      "   Predicted  Actual\n",
      "0  60.985007    53.0\n",
      "1  60.700462    67.0\n",
      "2  60.224452    62.0\n",
      "3  57.450462    58.0\n",
      "4  57.450462    61.0\n",
      "5  59.500462     0.0\n",
      "Average cross-validation error: 4.031834054834047\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    extract_info(i,files_dictionary_test,True)\n",
    "\n",
    "for i in range(1,11):\n",
    "    get_optimal_result(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
